{#
Copyright (c) Meta Platforms, Inc. and affiliates.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
#}

{{ triton_guidelines }}

TASK: Generate a complete Python implementation of a Triton kernel based on the following test requirements.

TEST CODE:
Analyze this test to understand what the kernel needs to implement:

```python
{{ test_code }}
```

FUSION PRIORITY:
- Inspect the problem description and test to identify sequences of operations that can run in a single pass.
- Prefer returning a single fused Triton kernel that covers the entire operator pipeline when technically feasible (e.g., combine matmul, bias addition, activations).
- Only fall back to an unfused implementation if fusion is provably impossible; in that case add a brief comment in the wrapper explaining the constraint.

RUNTIME RESTRICTIONS:
- The Python wrapper may only perform argument validation, shape/dtype checks, tensor allocation, and grid configuration.
- All mathematical work (convolutions, activations, pooling, reductions, etc.) must execute inside Triton kernels.
- Never import or instantiate `torch.nn` modules, call `torch.nn.functional` (or aliases such as `F.*`), or rely on PyTorch helpers like `torch.conv*`, `torch.tanh`, `torch.sigmoid`, `torch.max_pool*`, etc.
- Allowed PyTorch usage is limited to allocation helpers (`torch.empty_like`, `torch.zeros`, etc.), dtype/device checks, and assertions needed to launch the Triton kernel.

CRITICAL REQUIREMENTS - NO CHEATING ALLOWED:
1. The implementation must be a complete, valid Python file
2. Import necessary modules (triton, triton.language as tl, torch)
3. Create a Triton kernel (can be named anything, e.g., _kernel) decorated with @triton.jit
4. MUST create a wrapper function named 'kernel_function' that:
   - Takes normal Python/PyTorch arguments (tensors, scalars, etc.)
   - Handles grid calculation and kernel launch internally
   - Returns results as normal Python/PyTorch objects
   - This wrapper is what the test will import and call

**STRICTLY FORBIDDEN - DO NOT CHEAT:**
- DO NOT call PyTorch functions (torch.add, torch.mul, torch.sum, torch.matmul, torch.mm, torch.bmm, torch.einsum, etc.) inside the kernel computation
- DO NOT use PyTorch operations to perform the actual computation and just return the result (this includes tensor-tensor ops such as x.matmul(y), x.bmm(y), x.mm(y), or x.einsum(...))
- DO NOT implement the logic using pure PyTorch and avoid writing Triton kernel code
- The actual computation MUST be implemented using Triton operations (tl.load, tl.store, tl.sum, etc.)
- The wrapper function can use PyTorch for tensor creation, memory allocation, and result formatting, but the core computation MUST happen in the Triton kernel
- DO NOT import or instantiate torch.nn modules, call torch.nn.functional (including aliases like F.*), or use PyTorch activations/pooling helpers to satisfy the requirements

**WHAT IS REQUIRED:**
- Write actual Triton kernel code using triton.language operations
- Use tl.load() and tl.store() for memory access
- Use tl.sum(), tl.max(), tl.min() etc. for reductions  
- Use proper Triton indexing with tl.program_id(), tl.arange(), etc.
- Implement the algorithm step-by-step in Triton operations, not PyTorch shortcuts

5. Follow all Triton programming guidelines above
6. Include appropriate docstrings and comments
7. Handle edge cases and boundary conditions properly
8. Explicitly reason about what is being fused inside the wrapper docstring or leading comments so reviewers can confirm you attempted fusion
9. Keep the wrapper free of PyTorch compute primitivesâ€”no torch.nn modules, torch.nn.functional calls, or other high-level PyTorch operators in the execution path

{% if target_platform == "xpu" %}

## Intel XPU-Specific Optimizations

You are generating a Triton kernel for Intel XPU (Xe GPUs). Follow these guidelines:

1. **Device Context**: Use 'xpu' as the device instead of 'cuda'
2. **Memory Hierarchy**: Intel Xe has different cache sizes - optimize accordingly
3. **Thread Configuration**:
   - Subgroup size is typically 8, 16, or 32 (flexible)
   - num_warps: typically 4, 8, or 16 for Intel GPUs
   - BLOCK_SIZE: prefer 64, 128, 256, or 512
4. **Optimal Block Sizes**: Start with 128-256 for most kernels
5. **Data Types**: Intel supports fp32, fp16, bf16 (fp8 varies by generation)

{% endif %}

Example structure (wrapper can use PyTorch, but kernel must use Triton):
```python
import triton
import triton.language as tl
import torch

@triton.jit
def _actual_kernel(ptr_a, ptr_b, ptr_out, n_elements, BLOCK_SIZE: tl.constexpr):
    # MUST use Triton operations here, NOT PyTorch
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load using Triton
    a = tl.load(ptr_a + offsets, mask=mask)
    b = tl.load(ptr_b + offsets, mask=mask)
    
    # Compute using Triton operations (NOT torch.add!)
    result = a + b
    
    # Store using Triton
    tl.store(ptr_out + offsets, result, mask=mask)

def kernel_function(tensor_a, tensor_b):
    """Wrapper function that handles kernel launch."""
    # PyTorch operations allowed here for setup
    output = torch.empty_like(tensor_a)
    n_elements = tensor_a.numel()
    
    # Calculate grid and launch Triton kernel
    BLOCK_SIZE = 1024
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    _actual_kernel[grid](
        tensor_a, tensor_b, output, n_elements, BLOCK_SIZE
    )
    return output
```

Generate a complete kernel implementation with actual Triton code:
